# 关于YOLO(v3)是算法，你要知道的全在这里了  

*本文是我学习YOLO算法的理解总结，总结中不会谈及YOLO的发展过程，不会与其他对象检测算法进行对比介绍，也不会介绍YOLO9000相关的内容(会在文中阐述原因)，只单单总结目前YOLOv3的算法流程和实现细节。所以下文中所有的YOLO，如非特殊说明，均指YOLOv3。  
如果需要了解更多对象检测算法，可以通过参考这些论文链接：    
[R-CNN](https://arxiv.org/abs/1311.2524)  
[Fast R-CNN](https://arxiv.org/abs/1504.08083)  
[Faster R-CNN](https://arxiv.org/abs/1506.01497)  
[SSD](https://arxiv.org/abs/1512.02325)  
[YOLOv1](https://arxiv.org/abs/1506.02640)  
[YOLOv2](https://arxiv.org/abs/1612.08242)  
[YOLOv3](https://arxiv.org/abs/1804.02767)  
[RetinaNet](https://arxiv.org/abs/1708.02002)*

## 概述
在YOLO算法发表之前，大部分表现比较好的对象检测（Object Detection）算法都是以R-CNN为代表两阶段算法，这样的算法存在一个很明显的问题，那就是速度太慢，对于实时性要求很高的应用场景是不适用的。YOLO算法没有走优化算法第一阶段或者第二阶段的老路子，而是直接提出一步完成预测，而且是只在一个CNN网络中完成图片中所有位置对象的box和类别预测，其预测速度大大提升，完全可以满足实时对象检测。
YOLO算法的核心思想是创新性地提出了将输入图片进行N\*N的栅格化（每个小单元叫grid cell），然后将图片中某个对象的位置的训练拟合任务交与该对象中心位置所在的grid cell的bouding box。简单的理解的话，可以认为这也是一种很粗糙的region proposal，在训练的时候，我们通过grid cell的方式告诉模型，图片中对象A应该是由中心落在特定grid cell 的某个范围内的某些像素组成，模型接收到这些信息后就在grid cell周围以一定大小范围去寻找所有满足对象A的特征，经过很多次带惩罚的尝试训练后，它就能找到这个准确的范围了（说明不是瞎找），当然这个方位不仅是指长宽的大小范围，也包括小幅度的中心位置坐标变化，但是不管怎么变，中心位置不能越过该grid cell的范围。即时这样，也大大限制了模型在图片中瞎找时做的无用功。同时，一个grid cell的bounding box只负责预测一个对象，在训练时这个对象的类别是固定的，那么需要提取的特征也是特定的，这样就不会造成类别上的冲突，从而将位置检测和类别识别可以结合到一个CNN网络中预测。具体举例如下图。  

其实由上面的描述我们可以察觉出YOLO算法一个很大的缺陷，那就是对于挨得很近的两个对象，他们的中心可能位于同一个grid cell中，但是这个grid cell只负责预测一个对象，这时候就一定会舍弃掉一个对象，造成检测准确率下降。当然，这也是有解决方法的，那就是尽量减小图片栅格话的栅格单元尺寸，从而将这两个对象分隔到不同的grid cell中。  


以上是我个人理解的YOLO算法的核心思想，不管是YOLOv1还是v2、v3，其主要的核心还是以上所述，只是在box的拟合方式、骨干网络的设计、模型训练的稳定性、精度方面有所提升罢了。下面对整个模型的网络结构、训练实现细节进行阐述。

## 骨干网络（backbone network）
### batch normalization
### 残差模块

## 训练

### 训练时动态改变输入图片的大小

## 其他细节
